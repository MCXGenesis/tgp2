{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01842516",
   "metadata": {},
   "source": [
    "# Tugas Group Project 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ac1f10",
   "metadata": {},
   "source": [
    "- Rafindra Nabiel Fawwaz\n",
    "- Akhtar Zia Faizarobbi\n",
    "- Naufal Maula Nabil\n",
    "- Sinta Dewi Rahmawati"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf886d52",
   "metadata": {},
   "source": [
    "## Pemilahan Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610df4ef",
   "metadata": {},
   "source": [
    "### Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba18e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae61c7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('app_train_cleaned_encoded.csv')\n",
    "test_df = pd.read_csv('app_test_cleaned_encoded.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8995c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.shape\n",
    "test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0e5a6b",
   "metadata": {},
   "source": [
    "### Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a416b2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.drop(columns=['TARGET', 'SK_ID_CURR'])\n",
    "y = train_df['TARGET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebab5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.25, \n",
    "    stratify=y, \n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879fa80c",
   "metadata": {},
   "source": [
    "#### Statistik data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a934fa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Jumlah data sebelum split:\")\n",
    "print(y.value_counts(normalize=True))\n",
    "print()\n",
    "\n",
    "print(\"Jumlah data setelah split:\")\n",
    "print(\"Train set:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print()\n",
    "print(\"Test set:\")\n",
    "print(y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05b03d4",
   "metadata": {},
   "source": [
    "#### Kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ff37e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9045be",
   "metadata": {},
   "source": [
    "## Eksperimen Model Klasifikasi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff3708b",
   "metadata": {},
   "source": [
    "### 1. KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e048dd28",
   "metadata": {},
   "source": [
    "### 2. Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5031db97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import pandas as pd\n",
    "\n",
    "# Set up 10-fold cross-validation\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "nb_model = GaussianNB()\n",
    "\n",
    "# Untuk menyimpan metrik dari setiap fold\n",
    "results = {\n",
    "    'fold': [],\n",
    "    'accuracy': [],\n",
    "    'precision': [],\n",
    "    'recall': [],\n",
    "    'f1_score': [],\n",
    "    'auc_roc': []\n",
    "}\n",
    "\n",
    "# 10 kali training dan evaluasi\n",
    "for fold, (train_idx, val_idx) in enumerate(cv.split(X_train, y_train), 1):\n",
    "    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "    \n",
    "    # Train model\n",
    "    nb_model.fit(X_tr, y_tr)\n",
    "    \n",
    "    # Predict\n",
    "    y_val_pred = nb_model.predict(X_val)\n",
    "    y_val_proba = nb_model.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    # Hitung metrik\n",
    "    acc = accuracy_score(y_val, y_val_pred)\n",
    "    prec = precision_score(y_val, y_val_pred)\n",
    "    rec = recall_score(y_val, y_val_pred)\n",
    "    f1 = f1_score(y_val, y_val_pred)\n",
    "    auc = roc_auc_score(y_val, y_val_proba)\n",
    "    \n",
    "    # Simpan hasil\n",
    "    results['fold'].append(fold)\n",
    "    results['accuracy'].append(acc)\n",
    "    results['precision'].append(prec)\n",
    "    results['recall'].append(rec)\n",
    "    results['f1_score'].append(f1)\n",
    "    results['auc_roc'].append(auc)\n",
    "\n",
    "# Ubah ke DataFrame untuk analisis\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb54531",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report, confusion_matrix\n",
    "\n",
    "# Definisikan StratifiedKFold\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Ambil data training dan validasi dari fold ke-0\n",
    "train_idx, val_idx = list(cv.split(X_train, y_train))[0]\n",
    "X_fold_train, X_fold_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "y_fold_train, y_fold_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "# Inisialisasi dan fit model\n",
    "nb_model = GaussianNB()\n",
    "nb_model.fit(X_fold_train, y_fold_train)\n",
    "\n",
    "# Prediksi\n",
    "y_pred = nb_model.predict(X_fold_val)\n",
    "y_proba = nb_model.predict_proba(X_fold_val)[:, 1]\n",
    "\n",
    "# Evaluasi metrik\n",
    "acc = accuracy_score(y_fold_val, y_pred)\n",
    "prec = precision_score(y_fold_val, y_pred, zero_division=0)\n",
    "rec = recall_score(y_fold_val, y_pred, zero_division=0)\n",
    "f1 = f1_score(y_fold_val, y_pred, zero_division=0)\n",
    "auc = roc_auc_score(y_fold_val, y_proba)\n",
    "\n",
    "# Tampilkan hasil\n",
    "print(\"=== Evaluasi Model Naive Bayes (Fold ke-0) ===\")\n",
    "print(f\"Accuracy  : {acc:.6f}\")\n",
    "print(f\"Precision : {prec:.6f}\")\n",
    "print(f\"Recall    : {rec:.6f}\")\n",
    "print(f\"F1 Score  : {f1:.6f}\")\n",
    "print(f\"AUC-ROC   : {auc:.6f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_fold_val, y_pred, zero_division=0))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_fold_val, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209d0c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Misalnya X_train, y_train adalah data training yang sudah dipersiapkan\n",
    "model_naive_bayes = GaussianNB()\n",
    "\n",
    "# Melatih model dengan data training\n",
    "model_naive_bayes.fit(X_train, y_train)\n",
    "\n",
    "# Setelah model dilatih, gunakan model ini untuk prediksi\n",
    "y_test_pred = model_naive_bayes.predict(X_test) \n",
    "y_test_proba = model_naive_bayes.predict_proba(X_test)[:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb185b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediksi pada data test\n",
    "y_test_pred = model_naive_bayes.predict(X_test)  \n",
    "y_test_proba = model_naive_bayes.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Menghitung metrik evaluasi\n",
    "accuracy = accuracy_score(y_test, y_test_pred)\n",
    "precision = precision_score(y_test, y_test_pred, zero_division=0)\n",
    "recall = recall_score(y_test, y_test_pred, zero_division=0)\n",
    "f1 = f1_score(y_test, y_test_pred, zero_division=0)\n",
    "auc_roc = roc_auc_score(y_test, y_test_proba)\n",
    "\n",
    "# Menampilkan hasil\n",
    "print(f\"=== Evaluasi Model Naive Bayes pada Data Test ===\")\n",
    "print(f\"Accuracy  : {accuracy:.6f}\")\n",
    "print(f\"Precision : {precision:.6f}\")\n",
    "print(f\"Recall    : {recall:.6f}\")\n",
    "print(f\"F1 Score  : {f1:.6f}\")\n",
    "print(f\"AUC-ROC   : {auc_roc:.6f}\")\n",
    "\n",
    "# Menampilkan classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "# Menampilkan confusion matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f64f29f",
   "metadata": {},
   "source": [
    "#### HASIL EKSPERIMAN NAIVE BAYES \n",
    "\n",
    "Model Naive Bayes yang dilatih pada fold ke-0 menunjukkan performa yang sangat buruk, dengan akurasi rendah (~11%), precision rendah (~8%), dan AUC-ROC mendekati 0.5, yang menandakan model tidak mampu membedakan antara kelas. Meskipun recall tinggi (>96%), hal ini terjadi karena model cenderung memprediksi hampir semua data sebagai kelas positif, akibat dari ketidakseimbangan kelas yang signifikan. Secara keseluruhan, model ini tidak cocok digunakan pada dataset ini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084ace89",
   "metadata": {},
   "source": [
    "### 3. Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40b3aa9",
   "metadata": {},
   "source": [
    "### 4. SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f8c966",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbdbd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_list = ['linear', 'rbf']\n",
    "C_values = [0.1, 1, 10, 100, 1000]\n",
    "\n",
    "parameter_grid = [(k, c) for k in kernel_list for c in C_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c363ccd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "def evaluate_svm(kernel, C, train_idx, val_idx, X_train, y_train):\n",
    "    X_fold_train, X_fold_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_fold_train, y_fold_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "    # Buat dan fit model SVM\n",
    "    svm_model = SVC(kernel=kernel, C=C, probability=True, random_state=42)\n",
    "    svm_model.fit(X_fold_train, y_fold_train)\n",
    "    \n",
    "    # Prediksi\n",
    "    y_pred = svm_model.predict(X_fold_val)\n",
    "    y_proba = svm_model.predict_proba(X_fold_val)[:,1]\n",
    "    \n",
    "    # Hitung metrik\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_fold_val, y_pred),\n",
    "        'precision': precision_score(y_fold_val, y_pred, zero_division=0),\n",
    "        'recall': recall_score(y_fold_val, y_pred, zero_division=0),\n",
    "        'f1_score': f1_score(y_fold_val, y_pred, zero_division=0),\n",
    "        'auc_roc': roc_auc_score(y_fold_val, y_proba)\n",
    "    }\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef523ab2",
   "metadata": {},
   "source": [
    "#### Kombinasi 1: kernel = linear, C = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd6e688",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "kernel = 'linear'\n",
    "C = 0.1\n",
    "print(f\"\\nEvaluating SVM with kernel={kernel} and C={C}\")\n",
    "fold_metrics = Parallel(n_jobs=-1)(\n",
    "    delayed(evaluate_svm)(kernel, C, train_idx, val_idx, X_train, y_train)\n",
    "    for train_idx, val_idx in kfold.split(X_train, y_train)\n",
    ")\n",
    "fold_metrics_df = pd.DataFrame(fold_metrics)\n",
    "print(f\"Mean metrics:\\n{fold_metrics_df.mean()}\")\n",
    "print(f\"Std metrics:\\n{fold_metrics_df.std()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4baea8b2",
   "metadata": {},
   "source": [
    "#### Kombinasi 2: kernel = linear, C = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbea548",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = 'linear'\n",
    "C = 1\n",
    "print(f\"\\nEvaluating SVM with kernel={kernel} and C={C}\")\n",
    "fold_metrics = Parallel(n_jobs=-1)(\n",
    "    delayed(evaluate_svm)(kernel, C, train_idx, val_idx, X_train, y_train)\n",
    "    for train_idx, val_idx in kfold.split(X_train, y_train)\n",
    ")\n",
    "fold_metrics_df = pd.DataFrame(fold_metrics)\n",
    "print(f\"Mean metrics:\\n{fold_metrics_df.mean()}\")\n",
    "print(f\"Std metrics:\\n{fold_metrics_df.std()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bee743",
   "metadata": {},
   "source": [
    "#### Kombinasi 3: kernel = linear, C = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4339b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = 'linear'\n",
    "C = 10\n",
    "print(f\"\\nEvaluating SVM with kernel={kernel} and C={C}\")\n",
    "fold_metrics = Parallel(n_jobs=-1)(\n",
    "    delayed(evaluate_svm)(kernel, C, train_idx, val_idx, X_train, y_train)\n",
    "    for train_idx, val_idx in kfold.split(X_train, y_train)\n",
    ")\n",
    "fold_metrics_df = pd.DataFrame(fold_metrics)\n",
    "print(f\"Mean metrics:\\n{fold_metrics_df.mean()}\")\n",
    "print(f\"Std metrics:\\n{fold_metrics_df.std()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12461fcd",
   "metadata": {},
   "source": [
    "#### Kombinasi 4: kernel = linear, C = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e29610",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = 'linear'\n",
    "C = 100\n",
    "print(f\"\\nEvaluating SVM with kernel={kernel} and C={C}\")\n",
    "fold_metrics = Parallel(n_jobs=-1)(\n",
    "    delayed(evaluate_svm)(kernel, C, train_idx, val_idx, X_train, y_train)\n",
    "    for train_idx, val_idx in kfold.split(X_train, y_train)\n",
    ")\n",
    "fold_metrics_df = pd.DataFrame(fold_metrics)\n",
    "print(f\"Mean metrics:\\n{fold_metrics_df.mean()}\")\n",
    "print(f\"Std metrics:\\n{fold_metrics_df.std()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db95cde",
   "metadata": {},
   "source": [
    "#### Kombinasi 5: kernel = linear, C = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82734cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = 'linear'\n",
    "C = 1000\n",
    "print(f\"\\nEvaluating SVM with kernel={kernel} and C={C}\")\n",
    "fold_metrics = Parallel(n_jobs=-1)(\n",
    "    delayed(evaluate_svm)(kernel, C, train_idx, val_idx, X_train, y_train)\n",
    "    for train_idx, val_idx in kfold.split(X_train, y_train)\n",
    ")\n",
    "fold_metrics_df = pd.DataFrame(fold_metrics)\n",
    "print(f\"Mean metrics:\\n{fold_metrics_df.mean()}\")\n",
    "print(f\"Std metrics:\\n{fold_metrics_df.std()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10bd343",
   "metadata": {},
   "source": [
    "#### Kombinasi 6: kernel = rbf, C = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876e7ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = 'rbf'\n",
    "C = 0.1\n",
    "print(f\"\\nEvaluating SVM with kernel={kernel} and C={C}\")\n",
    "fold_metrics = Parallel(n_jobs=-1)(\n",
    "    delayed(evaluate_svm)(kernel, C, train_idx, val_idx, X_train, y_train)\n",
    "    for train_idx, val_idx in kfold.split(X_train, y_train)\n",
    ")\n",
    "fold_metrics_df = pd.DataFrame(fold_metrics)\n",
    "print(f\"Mean metrics:\\n{fold_metrics_df.mean()}\")\n",
    "print(f\"Std metrics:\\n{fold_metrics_df.std()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3343317",
   "metadata": {},
   "source": [
    "#### Kombinasi 7: kernel = linear, C = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e60b7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = 'rbf'\n",
    "C = 1\n",
    "print(f\"\\nEvaluating SVM with kernel={kernel} and C={C}\")\n",
    "fold_metrics = Parallel(n_jobs=-1)(\n",
    "    delayed(evaluate_svm)(kernel, C, train_idx, val_idx, X_train, y_train)\n",
    "    for train_idx, val_idx in kfold.split(X_train, y_train)\n",
    ")\n",
    "fold_metrics_df = pd.DataFrame(fold_metrics)\n",
    "print(f\"Mean metrics:\\n{fold_metrics_df.mean()}\")\n",
    "print(f\"Std metrics:\\n{fold_metrics_df.std()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48350013",
   "metadata": {},
   "source": [
    "#### Kombinasi 8: kernel = linear, C = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30eb360",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = 'rbf'\n",
    "C = 10\n",
    "print(f\"\\nEvaluating SVM with kernel={kernel} and C={C}\")\n",
    "fold_metrics = Parallel(n_jobs=-1)(\n",
    "    delayed(evaluate_svm)(kernel, C, train_idx, val_idx, X_train, y_train)\n",
    "    for train_idx, val_idx in kfold.split(X_train, y_train)\n",
    ")\n",
    "fold_metrics_df = pd.DataFrame(fold_metrics)\n",
    "print(f\"Mean metrics:\\n{fold_metrics_df.mean()}\")\n",
    "print(f\"Std metrics:\\n{fold_metrics_df.std()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee314fc",
   "metadata": {},
   "source": [
    "#### Kombinasi 9: kernel = rbf, C = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710e576b",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = 'rbf'\n",
    "C = 100\n",
    "print(f\"\\nEvaluating SVM with kernel={kernel} and C={C}\")\n",
    "fold_metrics = Parallel(n_jobs=-1)(\n",
    "    delayed(evaluate_svm)(kernel, C, train_idx, val_idx, X_train, y_train)\n",
    "    for train_idx, val_idx in kfold.split(X_train, y_train)\n",
    ")\n",
    "fold_metrics_df = pd.DataFrame(fold_metrics)\n",
    "print(f\"Mean metrics:\\n{fold_metrics_df.mean()}\")\n",
    "print(f\"Std metrics:\\n{fold_metrics_df.std()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7039606d",
   "metadata": {},
   "source": [
    "#### Kombinasi 10: kernel = linear, C = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65706db",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = 'rbf'\n",
    "C = 1000\n",
    "print(f\"\\nEvaluating SVM with kernel={kernel} and C={C}\")\n",
    "fold_metrics = Parallel(n_jobs=-1)(\n",
    "    delayed(evaluate_svm)(kernel, C, train_idx, val_idx, X_train, y_train)\n",
    "    for train_idx, val_idx in kfold.split(X_train, y_train)\n",
    ")\n",
    "fold_metrics_df = pd.DataFrame(fold_metrics)\n",
    "print(f\"Mean metrics:\\n{fold_metrics_df.mean()}\")\n",
    "print(f\"Std metrics:\\n{fold_metrics_df.std()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0575994c",
   "metadata": {},
   "source": [
    "#### Cari model terbaik berdasarkan mean f1-score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d6ba3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "best_model_info = results_df.sort_values(by='mean_f1_score', ascending=False).iloc[0]\n",
    "print(\"\\nModel terbaik berdasarkan hasil cross-validation:\")\n",
    "print(best_model_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004e7de3",
   "metadata": {},
   "source": [
    "#### Fit model terbaik ke seluruh data train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b531d042",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_kernel = best_model_info['kernel']\n",
    "best_C = best_model_info['C']\n",
    "\n",
    "final_svm_model = SVC(kernel=best_kernel, C=best_C, probability=True, random_state=42)\n",
    "final_svm_model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee9ea6b",
   "metadata": {},
   "source": [
    "#### Evaluasi di data train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc6caf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = final_svm_model.predict(X_train)\n",
    "y_train_proba = final_svm_model.predict_proba(X_train)[:,1]\n",
    "\n",
    "print(\"\\nEvaluasi model terbaik pada seluruh data train:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_train, y_train_pred)}\")\n",
    "print(f\"Precision: {precision_score(y_train, y_train_pred, zero_division=0)}\")\n",
    "print(f\"Recall: {recall_score(y_train, y_train_pred, zero_division=0)}\")\n",
    "print(f\"F1 Score: {f1_score(y_train, y_train_pred, zero_division=0)}\")\n",
    "print(f\"AUC ROC: {roc_auc_score(y_train, y_train_proba)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0692de2",
   "metadata": {},
   "source": [
    "#### Prediksi pada data test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5738492d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = final_svm_model.predict(X_test)\n",
    "y_test_proba = final_svm_model.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(\"\\nEvaluasi model terbaik pada data test:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_test_pred)}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_test_pred, zero_division=0)}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_test_pred, zero_division=0)}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_test_pred, zero_division=0)}\")\n",
    "print(f\"AUC ROC: {roc_auc_score(y_test, y_test_proba)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ec7cbf",
   "metadata": {},
   "source": [
    "### 5. Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bcf668",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt_model = DecisionTreeClassifier(random_state=1)\n",
    "dt_model.fit(X, y)\n",
    "y_pred = dt_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8c2ae8",
   "metadata": {},
   "source": [
    "#### Max Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e489b7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the range of max_depth values\n",
    "max_depths = np.linspace(1, 32, 32, endpoint=True, dtype=int)\n",
    "\n",
    "# Initialize lists to store results\n",
    "train_results = {'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'roc_auc': []}\n",
    "test_results = {'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'roc_auc': []}\n",
    "\n",
    "# Loop through each max_depth value\n",
    "for depth in max_depths:\n",
    "    dt = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
    "    dt.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions on training data\n",
    "    train_pred = dt.predict(X_train)\n",
    "    train_prob = dt.predict_proba(X_train)[:, 1]\n",
    "    \n",
    "    # Calculate metrics for training data\n",
    "    train_results['accuracy'].append(accuracy_score(y_train, train_pred))\n",
    "    train_results['precision'].append(precision_score(y_train, train_pred, zero_division=0))\n",
    "    train_results['recall'].append(recall_score(y_train, train_pred, zero_division=0))\n",
    "    train_results['f1'].append(f1_score(y_train, train_pred, zero_division=0))\n",
    "    train_results['roc_auc'].append(roc_auc_score(y_train, train_prob))\n",
    "    \n",
    "    # Predictions on testing data\n",
    "    test_pred = dt.predict(X_test)\n",
    "    test_prob = dt.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate metrics for testing data\n",
    "    test_results['accuracy'].append(accuracy_score(y_test, test_pred))\n",
    "    test_results['precision'].append(precision_score(y_test, test_pred, zero_division=0))\n",
    "    test_results['recall'].append(recall_score(y_test, test_pred, zero_division=0))\n",
    "    test_results['f1'].append(f1_score(y_test, test_pred, zero_division=0))\n",
    "    test_results['roc_auc'].append(roc_auc_score(y_test, test_prob))\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot ROC AUC\n",
    "plt.plot(max_depths, train_results['roc_auc'], 'b', label='Train ROC AUC')\n",
    "plt.plot(max_depths, test_results['roc_auc'], 'r', label='Test ROC AUC')\n",
    "\n",
    "# Plot Accuracy\n",
    "plt.plot(max_depths, train_results['accuracy'], 'g--', label='Train Accuracy')\n",
    "plt.plot(max_depths, test_results['accuracy'], 'y--', label='Test Accuracy')\n",
    "\n",
    "# Plot Precision\n",
    "plt.plot(max_depths, train_results['precision'], 'c-.', label='Train Precision')\n",
    "plt.plot(max_depths, test_results['precision'], 'm-.', label='Test Precision')\n",
    "\n",
    "# Plot Recall\n",
    "plt.plot(max_depths, train_results['recall'], 'k:', label='Train Recall')\n",
    "plt.plot(max_depths, test_results['recall'], 'orange', label='Test Recall')\n",
    "\n",
    "# Plot F1-Score\n",
    "plt.plot(max_depths, train_results['f1'], 'purple', label='Train F1-Score')\n",
    "plt.plot(max_depths, test_results['f1'], 'brown', label='Test F1-Score')\n",
    "\n",
    "# Add labels and legend\n",
    "plt.xlabel('Tree Depth')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Decision Tree Performance Metrics vs Tree Depth')\n",
    "plt.legend(loc='best')\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda097b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the range of max_depth values\n",
    "max_depths = [14, 15]\n",
    "\n",
    "# Initialize lists to store results\n",
    "train_results = {'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'roc_auc': []}\n",
    "test_results = {'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'roc_auc': []}\n",
    "\n",
    "# Loop through each max_depth value\n",
    "for depth in max_depths:\n",
    "    print(f\"\\nEvaluating max_depth={depth}\")\n",
    "    dt = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(X_train, y_train), 1):\n",
    "        # Split the data for the current fold\n",
    "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "        \n",
    "        # Train the model on the training fold\n",
    "        dt.fit(X_tr, y_tr)\n",
    "        \n",
    "        # Predictions on training fold\n",
    "        train_pred = dt.predict(X_tr)\n",
    "        train_prob = dt.predict_proba(X_tr)[:, 1]\n",
    "        \n",
    "        # Calculate metrics for training fold\n",
    "        train_accuracy = accuracy_score(y_tr, train_pred)\n",
    "        train_precision = precision_score(y_tr, train_pred, zero_division=0)\n",
    "        train_recall = recall_score(y_tr, train_pred, zero_division=0)\n",
    "        train_f1 = f1_score(y_tr, train_pred, zero_division=0)\n",
    "        train_roc_auc = roc_auc_score(y_tr, train_prob)\n",
    "        \n",
    "        # Store training metrics\n",
    "        train_results['accuracy'].append(train_accuracy)\n",
    "        train_results['precision'].append(train_precision)\n",
    "        train_results['recall'].append(train_recall)\n",
    "        train_results['f1'].append(train_f1)\n",
    "        train_results['roc_auc'].append(train_roc_auc)\n",
    "        \n",
    "        # Predictions on validation fold\n",
    "        val_pred = dt.predict(X_val)\n",
    "        val_prob = dt.predict_proba(X_val)[:, 1]\n",
    "        \n",
    "        # Calculate metrics for validation fold\n",
    "        val_accuracy = accuracy_score(y_val, val_pred)\n",
    "        val_precision = precision_score(y_val, val_pred, zero_division=0)\n",
    "        val_recall = recall_score(y_val, val_pred, zero_division=0)\n",
    "        val_f1 = f1_score(y_val, val_pred, zero_division=0)\n",
    "        val_roc_auc = roc_auc_score(y_val, val_prob)\n",
    "        \n",
    "        # Store validation metrics\n",
    "        test_results['accuracy'].append(val_accuracy)\n",
    "        test_results['precision'].append(val_precision)\n",
    "        test_results['recall'].append(val_recall)\n",
    "        test_results['f1'].append(val_f1)\n",
    "        test_results['roc_auc'].append(val_roc_auc)\n",
    "        \n",
    "        # Print metrics for the current fold\n",
    "        print(f\"Fold {fold}\")\n",
    "        print(f\"  Train Metrics: Accuracy={train_accuracy:.4f}, Precision={train_precision:.4f}, Recall={train_recall:.4f}, F1={train_f1:.4f}, ROC AUC={train_roc_auc:.4f}\")\n",
    "        print(f\"  Test Metrics:  Accuracy={val_accuracy:.4f}, Precision={val_precision:.4f}, Recall={val_recall:.4f}, F1={val_f1:.4f}, ROC AUC={val_roc_auc:.4f}\")\n",
    "\n",
    "    # Calculate and print mean and standard deviation for each metric\n",
    "    train_df = pd.DataFrame(train_results)\n",
    "    test_df = pd.DataFrame(test_results)\n",
    "\n",
    "    summary = {\n",
    "        'Train Mean': train_df.mean(),\n",
    "        'Train Std': train_df.std(),\n",
    "        'Test Mean': test_df.mean(),\n",
    "        'Test Std': test_df.std()\n",
    "    }\n",
    "\n",
    "    summary_df = pd.DataFrame(summary)\n",
    "    print(\"\\nSummary of 10-Fold Cross-Validation Metrics:\")\n",
    "    print(summary_df.round(4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefdb0c7",
   "metadata": {},
   "source": [
    "#### Min_samples_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235ca3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the range of min_samples_split values\n",
    "min_samples_splits = np.linspace(0.1, 1.0, 10, endpoint=True)\n",
    "\n",
    "# Initialize lists to store results\n",
    "train_results = {'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'roc_auc': []}\n",
    "test_results = {'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'roc_auc': []}\n",
    "\n",
    "# Loop through each min_samples_split value\n",
    "for min_samples_split in min_samples_splits:\n",
    "    dt = DecisionTreeClassifier(min_samples_split=int(min_samples_split * len(X_train)), random_state=42)\n",
    "    dt.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions on training data\n",
    "    train_pred = dt.predict(X_train)\n",
    "    train_prob = dt.predict_proba(X_train)[:, 1]\n",
    "    \n",
    "    # Calculate metrics for training data\n",
    "    train_results['accuracy'].append(accuracy_score(y_train, train_pred))\n",
    "    train_results['precision'].append(precision_score(y_train, train_pred, zero_division=0))\n",
    "    train_results['recall'].append(recall_score(y_train, train_pred, zero_division=0))\n",
    "    train_results['f1'].append(f1_score(y_train, train_pred, zero_division=0))\n",
    "    train_results['roc_auc'].append(roc_auc_score(y_train, train_prob))\n",
    "    \n",
    "    # Predictions on testing data\n",
    "    test_pred = dt.predict(X_test)\n",
    "    test_prob = dt.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate metrics for testing data\n",
    "    test_results['accuracy'].append(accuracy_score(y_test, test_pred))\n",
    "    test_results['precision'].append(precision_score(y_test, test_pred, zero_division=0))\n",
    "    test_results['recall'].append(recall_score(y_test, test_pred, zero_division=0))\n",
    "    test_results['f1'].append(f1_score(y_test, test_pred, zero_division=0))\n",
    "    test_results['roc_auc'].append(roc_auc_score(y_test, test_prob))\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot ROC AUC\n",
    "plt.plot(min_samples_splits, train_results['roc_auc'], 'b', label='Train ROC AUC')\n",
    "plt.plot(min_samples_splits, test_results['roc_auc'], 'r', label='Test ROC AUC')\n",
    "\n",
    "# Plot Accuracy\n",
    "plt.plot(min_samples_splits, train_results['accuracy'], 'g--', label='Train Accuracy')\n",
    "plt.plot(min_samples_splits, test_results['accuracy'], 'y--', label='Test Accuracy')\n",
    "\n",
    "# Plot Precision\n",
    "plt.plot(min_samples_splits, train_results['precision'], 'c-.', label='Train Precision')\n",
    "plt.plot(min_samples_splits, test_results['precision'], 'm-.', label='Test Precision')\n",
    "\n",
    "# Plot Recall\n",
    "plt.plot(min_samples_splits, train_results['recall'], 'k:', label='Train Recall')\n",
    "plt.plot(min_samples_splits, test_results['recall'], 'orange', label='Test Recall')\n",
    "\n",
    "# Plot F1-Score\n",
    "plt.plot(min_samples_splits, train_results['f1'], 'purple', label='Train F1-Score')\n",
    "plt.plot(min_samples_splits, test_results['f1'], 'brown', label='Test F1-Score')\n",
    "\n",
    "# Add labels and legend\n",
    "plt.xlabel('Min Samples Split (Fraction of Training Data)')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Decision Tree Performance Metrics vs Min Samples Split')\n",
    "plt.legend(loc='best')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3fe946",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "# Define the range of min_samples_split values\n",
    "min_samples_splits = [0.2, 0.4]\n",
    "\n",
    "# Initialize lists to store results\n",
    "train_results = {'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'roc_auc': []}\n",
    "test_results = {'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'roc_auc': []}\n",
    "\n",
    "# Loop through each min_samples_split value\n",
    "for min_samples_split in min_samples_splits:\n",
    "    print(f\"\\nEvaluating min_samples_split={min_samples_split}\")\n",
    "    dt = DecisionTreeClassifier(min_samples_split=int(min_samples_split * len(X_train)), random_state=42)\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(X_train, y_train), 1):\n",
    "        # Split the data for the current fold\n",
    "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "        \n",
    "        # Train the model on the training fold\n",
    "        dt.fit(X_tr, y_tr)\n",
    "        \n",
    "        # Predictions on training fold\n",
    "        train_pred = dt.predict(X_tr)\n",
    "        train_prob = dt.predict_proba(X_tr)[:, 1]\n",
    "        \n",
    "        # Calculate metrics for training fold\n",
    "        train_accuracy = accuracy_score(y_tr, train_pred)\n",
    "        train_precision = precision_score(y_tr, train_pred, zero_division=0)\n",
    "        train_recall = recall_score(y_tr, train_pred, zero_division=0)\n",
    "        train_f1 = f1_score(y_tr, train_pred, zero_division=0)\n",
    "        train_roc_auc = roc_auc_score(y_tr, train_prob)\n",
    "        \n",
    "        # Store training metrics\n",
    "        train_results['accuracy'].append(train_accuracy)\n",
    "        train_results['precision'].append(train_precision)\n",
    "        train_results['recall'].append(train_recall)\n",
    "        train_results['f1'].append(train_f1)\n",
    "        train_results['roc_auc'].append(train_roc_auc)\n",
    "        \n",
    "        # Predictions on validation fold\n",
    "        val_pred = dt.predict(X_val)\n",
    "        val_prob = dt.predict_proba(X_val)[:, 1]\n",
    "        \n",
    "        # Calculate metrics for validation fold\n",
    "        val_accuracy = accuracy_score(y_val, val_pred)\n",
    "        val_precision = precision_score(y_val, val_pred, zero_division=0)\n",
    "        val_recall = recall_score(y_val, val_pred, zero_division=0)\n",
    "        val_f1 = f1_score(y_val, val_pred, zero_division=0)\n",
    "        val_roc_auc = roc_auc_score(y_val, val_prob)\n",
    "        \n",
    "        # Store validation metrics\n",
    "        test_results['accuracy'].append(val_accuracy)\n",
    "        test_results['precision'].append(val_precision)\n",
    "        test_results['recall'].append(val_recall)\n",
    "        test_results['f1'].append(val_f1)\n",
    "        test_results['roc_auc'].append(val_roc_auc)\n",
    "        \n",
    "        # Print metrics for the current fold\n",
    "        print(f\"Fold {fold}\")\n",
    "        print(f\"  Train Metrics: Accuracy={train_accuracy:.4f}, Precision={train_precision:.4f}, Recall={train_recall:.4f}, F1={train_f1:.4f}, ROC AUC={train_roc_auc:.4f}\")\n",
    "        print(f\"  Test Metrics:  Accuracy={val_accuracy:.4f}, Precision={val_precision:.4f}, Recall={val_recall:.4f}, F1={val_f1:.4f}, ROC AUC={val_roc_auc:.4f}\")\n",
    "\n",
    "    # Calculate and print mean and standard deviation for each metric\n",
    "    train_df = pd.DataFrame(train_results)\n",
    "    test_df = pd.DataFrame(test_results)\n",
    "\n",
    "    summary = {\n",
    "        'Train Mean': train_df.mean(),\n",
    "        'Train Std': train_df.std(),\n",
    "        'Test Mean': test_df.mean(),\n",
    "        'Test Std': test_df.std()\n",
    "    }\n",
    "\n",
    "    summary_df = pd.DataFrame(summary)\n",
    "    print(\"\\nSummary of 10-Fold Cross-Validation Metrics:\")\n",
    "    print(summary_df.round(4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab94511e",
   "metadata": {},
   "source": [
    "#### Min samples leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606d94cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the range of min_samples_leaf values\n",
    "min_samples_leafs = np.linspace(0.1, 0.5, 5, endpoint=True)\n",
    "\n",
    "# Initialize lists to store results\n",
    "train_results = {'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'roc_auc': []}\n",
    "test_results = {'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'roc_auc': []}\n",
    "\n",
    "# Loop through each min_samples_leaf value\n",
    "for min_samples_leaf in min_samples_leafs:\n",
    "    dt = DecisionTreeClassifier(min_samples_leaf=int(min_samples_leaf * len(X_train)), random_state=42)\n",
    "    dt.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions on training data\n",
    "    train_pred = dt.predict(X_train)\n",
    "    train_prob = dt.predict_proba(X_train)[:, 1]\n",
    "    \n",
    "    # Calculate metrics for training data\n",
    "    train_results['accuracy'].append(accuracy_score(y_train, train_pred))\n",
    "    train_results['precision'].append(precision_score(y_train, train_pred, zero_division=0))\n",
    "    train_results['recall'].append(recall_score(y_train, train_pred, zero_division=0))\n",
    "    train_results['f1'].append(f1_score(y_train, train_pred, zero_division=0))\n",
    "    train_results['roc_auc'].append(roc_auc_score(y_train, train_prob))\n",
    "    \n",
    "    # Predictions on testing data\n",
    "    test_pred = dt.predict(X_test)\n",
    "    test_prob = dt.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate metrics for testing data\n",
    "    test_results['accuracy'].append(accuracy_score(y_test, test_pred))\n",
    "    test_results['precision'].append(precision_score(y_test, test_pred, zero_division=0))\n",
    "    test_results['recall'].append(recall_score(y_test, test_pred, zero_division=0))\n",
    "    test_results['f1'].append(f1_score(y_test, test_pred, zero_division=0))\n",
    "    test_results['roc_auc'].append(roc_auc_score(y_test, test_prob))\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot ROC AUC\n",
    "plt.plot(min_samples_leafs, train_results['roc_auc'], 'b', label='Train ROC AUC')\n",
    "plt.plot(min_samples_leafs, test_results['roc_auc'], 'r', label='Test ROC AUC')\n",
    "\n",
    "# Plot Accuracy\n",
    "plt.plot(min_samples_leafs, train_results['accuracy'], 'g--', label='Train Accuracy')\n",
    "plt.plot(min_samples_leafs, test_results['accuracy'], 'y--', label='Test Accuracy')\n",
    "\n",
    "# Plot Precision\n",
    "plt.plot(min_samples_leafs, train_results['precision'], 'c-.', label='Train Precision')\n",
    "plt.plot(min_samples_leafs, test_results['precision'], 'm-.', label='Test Precision')\n",
    "\n",
    "# Plot Recall\n",
    "plt.plot(min_samples_leafs, train_results['recall'], 'k:', label='Train Recall')\n",
    "plt.plot(min_samples_leafs, test_results['recall'], 'orange', label='Test Recall')\n",
    "\n",
    "# Plot F1-Score\n",
    "plt.plot(min_samples_leafs, train_results['f1'], 'purple', label='Train F1-Score')\n",
    "plt.plot(min_samples_leafs, test_results['f1'], 'brown', label='Test F1-Score')\n",
    "\n",
    "# Add labels and legend\n",
    "plt.xlabel('Min Samples Leaf (Fraction of Training Data)')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Decision Tree Performance Metrics vs Min Samples Leaf')\n",
    "plt.legend(loc='best')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8850a62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "# Define the range of min_samples_leaf values\n",
    "min_samples_leafs = [0.2, 0.3]\n",
    "\n",
    "# Initialize lists to store results\n",
    "train_results = {'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'roc_auc': []}\n",
    "test_results = {'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'roc_auc': []}\n",
    "\n",
    "# Loop through each min_samples_leaf value\n",
    "for min_samples_leaf in min_samples_leafs:\n",
    "    print(f\"\\nEvaluating min_samples_leaf={min_samples_leaf}\")\n",
    "    dt = DecisionTreeClassifier(min_samples_leaf=int(min_samples_leaf * len(X_train)), random_state=42)\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(X_train, y_train), 1):\n",
    "        # Split the data for the current fold\n",
    "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "        \n",
    "        # Train the model on the training fold\n",
    "        dt.fit(X_tr, y_tr)\n",
    "        \n",
    "        # Predictions on training fold\n",
    "        train_pred = dt.predict(X_tr)\n",
    "        train_prob = dt.predict_proba(X_tr)[:, 1]\n",
    "        \n",
    "        # Calculate metrics for training fold\n",
    "        train_accuracy = accuracy_score(y_tr, train_pred)\n",
    "        train_precision = precision_score(y_tr, train_pred, zero_division=0)\n",
    "        train_recall = recall_score(y_tr, train_pred, zero_division=0)\n",
    "        train_f1 = f1_score(y_tr, train_pred, zero_division=0)\n",
    "        train_roc_auc = roc_auc_score(y_tr, train_prob)\n",
    "        \n",
    "        # Store training metrics\n",
    "        train_results['accuracy'].append(train_accuracy)\n",
    "        train_results['precision'].append(train_precision)\n",
    "        train_results['recall'].append(train_recall)\n",
    "        train_results['f1'].append(train_f1)\n",
    "        train_results['roc_auc'].append(train_roc_auc)\n",
    "        \n",
    "        # Predictions on validation fold\n",
    "        val_pred = dt.predict(X_val)\n",
    "        val_prob = dt.predict_proba(X_val)[:, 1]\n",
    "        \n",
    "        # Calculate metrics for validation fold\n",
    "        val_accuracy = accuracy_score(y_val, val_pred)\n",
    "        val_precision = precision_score(y_val, val_pred, zero_division=0)\n",
    "        val_recall = recall_score(y_val, val_pred, zero_division=0)\n",
    "        val_f1 = f1_score(y_val, val_pred, zero_division=0)\n",
    "        val_roc_auc = roc_auc_score(y_val, val_prob)\n",
    "        \n",
    "        # Store validation metrics\n",
    "        test_results['accuracy'].append(val_accuracy)\n",
    "        test_results['precision'].append(val_precision)\n",
    "        test_results['recall'].append(val_recall)\n",
    "        test_results['f1'].append(val_f1)\n",
    "        test_results['roc_auc'].append(val_roc_auc)\n",
    "        \n",
    "        # Print metrics for the current fold\n",
    "        print(f\"Fold {fold}\")\n",
    "        print(f\"  Train Metrics: Accuracy={train_accuracy:.4f}, Precision={train_precision:.4f}, Recall={train_recall:.4f}, F1={train_f1:.4f}, ROC AUC={train_roc_auc:.4f}\")\n",
    "        print(f\"  Test Metrics:  Accuracy={val_accuracy:.4f}, Precision={val_precision:.4f}, Recall={val_recall:.4f}, F1={val_f1:.4f}, ROC AUC={val_roc_auc:.4f}\")\n",
    "\n",
    "    # Calculate and print mean and standard deviation for each metric\n",
    "    train_df = pd.DataFrame(train_results)\n",
    "    test_df = pd.DataFrame(test_results)\n",
    "\n",
    "    summary = {\n",
    "        'Train Mean': train_df.mean(),\n",
    "        'Train Std': train_df.std(),\n",
    "        'Test Mean': test_df.mean(),\n",
    "        'Test Std': test_df.std()\n",
    "    }\n",
    "\n",
    "    summary_df = pd.DataFrame(summary)\n",
    "    print(\"\\nSummary of 10-Fold Cross-Validation Metrics:\")\n",
    "    print(summary_df.round(4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffb6eef",
   "metadata": {},
   "source": [
    "#### Max Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0160a6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the range of max_features values\n",
    "max_features = list(range(1, X_train.shape[1] + 1))\n",
    "\n",
    "# Initialize lists to store results\n",
    "train_results = {'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'roc_auc': []}\n",
    "test_results = {'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'roc_auc': []}\n",
    "\n",
    "# Loop through each max_features value\n",
    "for max_feature in max_features:\n",
    "    dt = DecisionTreeClassifier(max_features=max_feature, random_state=42)\n",
    "    dt.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions on training data\n",
    "    train_pred = dt.predict(X_train)\n",
    "    train_prob = dt.predict_proba(X_train)[:, 1]\n",
    "    \n",
    "    # Calculate metrics for training data\n",
    "    train_results['accuracy'].append(accuracy_score(y_train, train_pred))\n",
    "    train_results['precision'].append(precision_score(y_train, train_pred, zero_division=0))\n",
    "    train_results['recall'].append(recall_score(y_train, train_pred, zero_division=0))\n",
    "    train_results['f1'].append(f1_score(y_train, train_pred, zero_division=0))\n",
    "    train_results['roc_auc'].append(roc_auc_score(y_train, train_prob))\n",
    "    \n",
    "    # Predictions on testing data\n",
    "    test_pred = dt.predict(X_test)\n",
    "    test_prob = dt.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate metrics for testing data\n",
    "    test_results['accuracy'].append(accuracy_score(y_test, test_pred))\n",
    "    test_results['precision'].append(precision_score(y_test, test_pred, zero_division=0))\n",
    "    test_results['recall'].append(recall_score(y_test, test_pred, zero_division=0))\n",
    "    test_results['f1'].append(f1_score(y_test, test_pred, zero_division=0))\n",
    "    test_results['roc_auc'].append(roc_auc_score(y_test, test_prob))\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot ROC AUC\n",
    "plt.plot(max_features, train_results['roc_auc'], 'b', label='Train ROC AUC')\n",
    "plt.plot(max_features, test_results['roc_auc'], 'r', label='Test ROC AUC')\n",
    "\n",
    "# Plot Accuracy\n",
    "plt.plot(max_features, train_results['accuracy'], 'g--', label='Train Accuracy')\n",
    "plt.plot(max_features, test_results['accuracy'], 'y--', label='Test Accuracy')\n",
    "\n",
    "# Plot Precision\n",
    "plt.plot(max_features, train_results['precision'], 'c-.', label='Train Precision')\n",
    "plt.plot(max_features, test_results['precision'], 'm-.', label='Test Precision')\n",
    "\n",
    "# Plot Recall\n",
    "plt.plot(max_features, train_results['recall'], 'k:', label='Train Recall')\n",
    "plt.plot(max_features, test_results['recall'], 'orange', label='Test Recall')\n",
    "\n",
    "# Plot F1-Score\n",
    "plt.plot(max_features, train_results['f1'], 'purple', label='Train F1-Score')\n",
    "plt.plot(max_features, test_results['f1'], 'brown', label='Test F1-Score')\n",
    "\n",
    "# Add labels and legend\n",
    "plt.xlabel('Max Features')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Decision Tree Performance Metrics vs Max Features')\n",
    "plt.legend(loc='best')\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a957598",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Define the range of max_features values\n",
    "max_features_values = [10, 'sqrt']\n",
    "\n",
    "# Initialize lists to store results\n",
    "train_results = {'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'roc_auc': []}\n",
    "test_results = {'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'roc_auc': []}\n",
    "\n",
    "# Loop through each max_features value\n",
    "for max_features in max_features_values:\n",
    "    print(f\"\\nEvaluating max_features={max_features}\")\n",
    "    dt = DecisionTreeClassifier(max_features=max_features, random_state=42)\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(X_train, y_train), 1):\n",
    "        # Split the data for the current fold\n",
    "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "        \n",
    "        # Train the model on the training fold\n",
    "        dt.fit(X_tr, y_tr)\n",
    "        \n",
    "        # Predictions on training fold\n",
    "        train_pred = dt.predict(X_tr)\n",
    "        train_prob = dt.predict_proba(X_tr)[:, 1]\n",
    "        \n",
    "        # Calculate metrics for training fold\n",
    "        train_accuracy = accuracy_score(y_tr, train_pred)\n",
    "        train_precision = precision_score(y_tr, train_pred, zero_division=0)\n",
    "        train_recall = recall_score(y_tr, train_pred, zero_division=0)\n",
    "        train_f1 = f1_score(y_tr, train_pred, zero_division=0)\n",
    "        train_roc_auc = roc_auc_score(y_tr, train_prob)\n",
    "        \n",
    "        # Store training metrics\n",
    "        train_results['accuracy'].append(train_accuracy)\n",
    "        train_results['precision'].append(train_precision)\n",
    "        train_results['recall'].append(train_recall)\n",
    "        train_results['f1'].append(train_f1)\n",
    "        train_results['roc_auc'].append(train_roc_auc)\n",
    "        \n",
    "        # Predictions on validation fold\n",
    "        val_pred = dt.predict(X_val)\n",
    "        val_prob = dt.predict_proba(X_val)[:, 1]\n",
    "        \n",
    "        # Calculate metrics for validation fold\n",
    "        val_accuracy = accuracy_score(y_val, val_pred)\n",
    "        val_precision = precision_score(y_val, val_pred, zero_division=0)\n",
    "        val_recall = recall_score(y_val, val_pred, zero_division=0)\n",
    "        val_f1 = f1_score(y_val, val_pred, zero_division=0)\n",
    "        val_roc_auc = roc_auc_score(y_val, val_prob)\n",
    "        \n",
    "        # Store validation metrics\n",
    "        test_results['accuracy'].append(val_accuracy)\n",
    "        test_results['precision'].append(val_precision)\n",
    "        test_results['recall'].append(val_recall)\n",
    "        test_results['f1'].append(val_f1)\n",
    "        test_results['roc_auc'].append(val_roc_auc)\n",
    "        \n",
    "        # Print metrics for the current fold\n",
    "        print(f\"Fold {fold}\")\n",
    "        print(f\"  Train Metrics: Accuracy={train_accuracy:.4f}, Precision={train_precision:.4f}, Recall={train_recall:.4f}, F1={train_f1:.4f}, ROC AUC={train_roc_auc:.4f}\")\n",
    "        print(f\"  Test Metrics:  Accuracy={val_accuracy:.4f}, Precision={val_precision:.4f}, Recall={val_recall:.4f}, F1={val_f1:.4f}, ROC AUC={val_roc_auc:.4f}\")\n",
    "\n",
    "         # Calculate and print mean and standard deviation for each metric\n",
    "    train_df = pd.DataFrame(train_results)\n",
    "    test_df = pd.DataFrame(test_results)\n",
    "\n",
    "    summary = {\n",
    "        'Train Mean': train_df.mean(),\n",
    "        'Train Std': train_df.std(),\n",
    "        'Test Mean': test_df.mean(),\n",
    "        'Test Std': test_df.std()\n",
    "    }\n",
    "\n",
    "    summary_df = pd.DataFrame(summary)\n",
    "    print(\"\\nSummary of 10-Fold Cross-Validation Metrics:\")\n",
    "    print(summary_df.round(4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee093bb",
   "metadata": {},
   "source": [
    "#### Max Leaf Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd901f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the range of max_leaf_nodes values\n",
    "max_leaf_nodes = np.linspace(2, 32, 31, endpoint=True, dtype=int)\n",
    "\n",
    "# Initialize lists to store results\n",
    "train_results = {'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'roc_auc': []}\n",
    "test_results = {'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'roc_auc': []}\n",
    "\n",
    "# Loop through each max_leaf_nodes value\n",
    "for max_leaf_node in max_leaf_nodes:\n",
    "    dt = DecisionTreeClassifier(max_leaf_nodes=max_leaf_node, random_state=42)\n",
    "    dt.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions on training data\n",
    "    train_pred = dt.predict(X_train)\n",
    "    train_prob = dt.predict_proba(X_train)[:, 1]\n",
    "    \n",
    "    # Calculate metrics for training data\n",
    "    train_results['accuracy'].append(accuracy_score(y_train, train_pred))\n",
    "    train_results['precision'].append(precision_score(y_train, train_pred, zero_division=0))\n",
    "    train_results['recall'].append(recall_score(y_train, train_pred, zero_division=0))\n",
    "    train_results['f1'].append(f1_score(y_train, train_pred, zero_division=0))\n",
    "    train_results['roc_auc'].append(roc_auc_score(y_train, train_prob))\n",
    "    \n",
    "    # Predictions on testing data\n",
    "    test_pred = dt.predict(X_test)\n",
    "    test_prob = dt.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate metrics for testing data\n",
    "    test_results['accuracy'].append(accuracy_score(y_test, test_pred))\n",
    "    test_results['precision'].append(precision_score(y_test, test_pred, zero_division=0))\n",
    "    test_results['recall'].append(recall_score(y_test, test_pred, zero_division=0))\n",
    "    test_results['f1'].append(f1_score(y_test, test_pred, zero_division=0))\n",
    "    test_results['roc_auc'].append(roc_auc_score(y_test, test_prob))\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot ROC AUC\n",
    "plt.plot(max_leaf_nodes, train_results['roc_auc'], 'b', label='Train ROC AUC')\n",
    "plt.plot(max_leaf_nodes, test_results['roc_auc'], 'r', label='Test ROC AUC')\n",
    "\n",
    "# Plot Accuracy\n",
    "plt.plot(max_leaf_nodes, train_results['accuracy'], 'g--', label='Train Accuracy')\n",
    "plt.plot(max_leaf_nodes, test_results['accuracy'], 'y--', label='Test Accuracy')\n",
    "\n",
    "# Plot Precision\n",
    "plt.plot(max_leaf_nodes, train_results['precision'], 'c-.', label='Train Precision')\n",
    "plt.plot(max_leaf_nodes, test_results['precision'], 'm-.', label='Test Precision')\n",
    "\n",
    "# Plot Recall\n",
    "plt.plot(max_leaf_nodes, train_results['recall'], 'k:', label='Train Recall')\n",
    "plt.plot(max_leaf_nodes, test_results['recall'], 'orange', label='Test Recall')\n",
    "\n",
    "# Plot F1-Score\n",
    "plt.plot(max_leaf_nodes, train_results['f1'], 'purple', label='Train F1-Score')\n",
    "plt.plot(max_leaf_nodes, test_results['f1'], 'brown', label='Test F1-Score')\n",
    "\n",
    "# Add labels and legend\n",
    "plt.xlabel('Max Leaf Nodes')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Decision Tree Performance Metrics vs Max Leaf Nodes')\n",
    "plt.legend(loc='best')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9653316d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Define the range of max_leaf_nodes values\n",
    "max_leaf_nodes_values = [20, 25]\n",
    "\n",
    "# Initialize lists to store results\n",
    "train_results = {'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'roc_auc': []}\n",
    "test_results = {'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'roc_auc': []}\n",
    "\n",
    "# Loop through each max_leaf_nodes value\n",
    "for max_leaf_nodes in max_leaf_nodes_values:\n",
    "    print(f\"\\nEvaluating max_leaf_nodes={max_leaf_nodes}\")\n",
    "    dt = DecisionTreeClassifier(max_leaf_nodes=max_leaf_nodes, random_state=42)\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(X_train, y_train), 1):\n",
    "        # Split the data for the current fold\n",
    "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "        \n",
    "        # Train the model on the training fold\n",
    "        dt.fit(X_tr, y_tr)\n",
    "        \n",
    "        # Predictions on training fold\n",
    "        train_pred = dt.predict(X_tr)\n",
    "        train_prob = dt.predict_proba(X_tr)[:, 1]\n",
    "        \n",
    "        # Calculate metrics for training fold\n",
    "        train_accuracy = accuracy_score(y_tr, train_pred)\n",
    "        train_precision = precision_score(y_tr, train_pred, zero_division=0)\n",
    "        train_recall = recall_score(y_tr, train_pred, zero_division=0)\n",
    "        train_f1 = f1_score(y_tr, train_pred, zero_division=0)\n",
    "        train_roc_auc = roc_auc_score(y_tr, train_prob)\n",
    "        \n",
    "        # Store training metrics\n",
    "        train_results['accuracy'].append(train_accuracy)\n",
    "        train_results['precision'].append(train_precision)\n",
    "        train_results['recall'].append(train_recall)\n",
    "        train_results['f1'].append(train_f1)\n",
    "        train_results['roc_auc'].append(train_roc_auc)\n",
    "        \n",
    "        # Predictions on validation fold\n",
    "        val_pred = dt.predict(X_val)\n",
    "        val_prob = dt.predict_proba(X_val)[:, 1]\n",
    "        \n",
    "        # Calculate metrics for validation fold\n",
    "        val_accuracy = accuracy_score(y_val, val_pred)\n",
    "        val_precision = precision_score(y_val, val_pred, zero_division=0)\n",
    "        val_recall = recall_score(y_val, val_pred, zero_division=0)\n",
    "        val_f1 = f1_score(y_val, val_pred, zero_division=0)\n",
    "        val_roc_auc = roc_auc_score(y_val, val_prob)\n",
    "        \n",
    "        # Store validation metrics\n",
    "        test_results['accuracy'].append(val_accuracy)\n",
    "        test_results['precision'].append(val_precision)\n",
    "        test_results['recall'].append(val_recall)\n",
    "        test_results['f1'].append(val_f1)\n",
    "        test_results['roc_auc'].append(val_roc_auc)\n",
    "        \n",
    "        # Print metrics for the current fold\n",
    "        print(f\"Fold {fold}\")\n",
    "        print(f\"  Train Metrics: Accuracy={train_accuracy:.4f}, Precision={train_precision:.4f}, Recall={train_recall:.4f}, F1={train_f1:.4f}, ROC AUC={train_roc_auc:.4f}\")\n",
    "        print(f\"  Test Metrics:  Accuracy={val_accuracy:.4f}, Precision={val_precision:.4f}, Recall={val_recall:.4f}, F1={val_f1:.4f}, ROC AUC={val_roc_auc:.4f}\")\n",
    "\n",
    "     # Calculate and print mean and standard deviation for each metric\n",
    "    train_df = pd.DataFrame(train_results)\n",
    "    test_df = pd.DataFrame(test_results)\n",
    "\n",
    "    summary = {\n",
    "        'Train Mean': train_df.mean(),\n",
    "        'Train Std': train_df.std(),\n",
    "        'Test Mean': test_df.mean(),\n",
    "        'Test Std': test_df.std()\n",
    "    }\n",
    "\n",
    "    summary_df = pd.DataFrame(summary)\n",
    "    print(\"\\nSummary of 10-Fold Cross-Validation Metrics:\")\n",
    "    print(summary_df.round(4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553c8a45",
   "metadata": {},
   "source": [
    "#### Criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325acb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the range of criterion values\n",
    "criteria = ['gini', 'entropy']\n",
    "\n",
    "# Initialize lists to store results\n",
    "train_results = {'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'roc_auc': []}\n",
    "test_results = {'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'roc_auc': []}\n",
    "\n",
    "# Loop through each criterion value\n",
    "for criterion in criteria:\n",
    "    dt = DecisionTreeClassifier(criterion=criterion, random_state=42)\n",
    "    dt.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions on training data\n",
    "    train_pred = dt.predict(X_train)\n",
    "    train_prob = dt.predict_proba(X_train)[:, 1]\n",
    "    \n",
    "    # Calculate metrics for training data\n",
    "    train_results['accuracy'].append(accuracy_score(y_train, train_pred))\n",
    "    train_results['precision'].append(precision_score(y_train, train_pred, zero_division=0))\n",
    "    train_results['recall'].append(recall_score(y_train, train_pred, zero_division=0))\n",
    "    train_results['f1'].append(f1_score(y_train, train_pred, zero_division=0))\n",
    "    train_results['roc_auc'].append(roc_auc_score(y_train, train_prob))\n",
    "    \n",
    "    # Predictions on testing data\n",
    "    test_pred = dt.predict(X_test)\n",
    "    test_prob = dt.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate metrics for testing data\n",
    "    test_results['accuracy'].append(accuracy_score(y_test, test_pred))\n",
    "    test_results['precision'].append(precision_score(y_test, test_pred, zero_division=0))\n",
    "    test_results['recall'].append(recall_score(y_test, test_pred, zero_division=0))\n",
    "    test_results['f1'].append(f1_score(y_test, test_pred, zero_division=0))\n",
    "    test_results['roc_auc'].append(roc_auc_score(y_test, test_prob))\n",
    "\n",
    "# Plot the results\n",
    "x = np.arange(len(criteria))  # X-axis positions for the criteria\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot ROC AUC\n",
    "plt.bar(x - 0.3, train_results['roc_auc'], width=0.2, label='Train ROC AUC', color='b')\n",
    "plt.bar(x - 0.1, test_results['roc_auc'], width=0.2, label='Test ROC AUC', color='r')\n",
    "\n",
    "# Plot Accuracy\n",
    "plt.bar(x + 0.1, train_results['accuracy'], width=0.2, label='Train Accuracy', color='g')\n",
    "plt.bar(x + 0.3, test_results['accuracy'], width=0.2, label='Test Accuracy', color='y')\n",
    "\n",
    "# Add labels and legend\n",
    "plt.xticks(x, criteria)\n",
    "plt.xlabel('Criterion')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Decision Tree Performance Metrics vs Criterion')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84adf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the range of criterion values\n",
    "criteria = ['gini', 'entropy']\n",
    "\n",
    "# Initialize lists to store results\n",
    "train_results = {'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'roc_auc': []}\n",
    "test_results = {'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'roc_auc': []}\n",
    "\n",
    "# Loop through each criterion value\n",
    "for criterion in criteria:\n",
    "    print(f\"\\nEvaluating criterion={criterion}\")\n",
    "    dt = DecisionTreeClassifier(criterion=criterion, random_state=42)\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(X_train, y_train), 1):\n",
    "        # Split the data for the current fold\n",
    "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "        \n",
    "        # Train the model on the training fold\n",
    "        dt.fit(X_tr, y_tr)\n",
    "        \n",
    "        # Predictions on training fold\n",
    "        train_pred = dt.predict(X_tr)\n",
    "        train_prob = dt.predict_proba(X_tr)[:, 1]\n",
    "        \n",
    "        # Calculate metrics for training fold\n",
    "        train_results['accuracy'].append(accuracy_score(y_tr, train_pred))\n",
    "        train_results['precision'].append(precision_score(y_tr, train_pred, zero_division=0))\n",
    "        train_results['recall'].append(recall_score(y_tr, train_pred, zero_division=0))\n",
    "        train_results['f1'].append(f1_score(y_tr, train_pred, zero_division=0))\n",
    "        train_results['roc_auc'].append(roc_auc_score(y_tr, train_prob))\n",
    "        \n",
    "        # Predictions on validation fold\n",
    "        val_pred = dt.predict(X_val)\n",
    "        val_prob = dt.predict_proba(X_val)[:, 1]\n",
    "        \n",
    "        # Calculate metrics for validation fold\n",
    "        test_results['accuracy'].append(accuracy_score(y_val, val_pred))\n",
    "        test_results['precision'].append(precision_score(y_val, val_pred, zero_division=0))\n",
    "        test_results['recall'].append(recall_score(y_val, val_pred, zero_division=0))\n",
    "        test_results['f1'].append(f1_score(y_val, val_pred, zero_division=0))\n",
    "        test_results['roc_auc'].append(roc_auc_score(y_val, val_prob))\n",
    "        \n",
    "        # Print metrics for the current fold\n",
    "        print(f\"Fold {fold}\")\n",
    "        print(f\"  Train Metrics: Accuracy={train_results['accuracy'][-1]:.4f}, Precision={train_results['precision'][-1]:.4f}, Recall={train_results['recall'][-1]:.4f}, F1={train_results['f1'][-1]:.4f}, ROC AUC={train_results['roc_auc'][-1]:.4f}\")\n",
    "        print(f\"  Test Metrics:  Accuracy={test_results['accuracy'][-1]:.4f}, Precision={test_results['precision'][-1]:.4f}, Recall={test_results['recall'][-1]:.4f}, F1={test_results['f1'][-1]:.4f}, ROC AUC={test_results['roc_auc'][-1]:.4f}\")\n",
    "\n",
    "    # Calculate and print mean and standard deviation for each metric\n",
    "    train_df = pd.DataFrame(train_results)\n",
    "    test_df = pd.DataFrame(test_results)\n",
    "\n",
    "    summary = {\n",
    "        'Train Mean': train_df.mean(),\n",
    "        'Train Std': train_df.std(),\n",
    "        'Test Mean': test_df.mean(),\n",
    "        'Test Std': test_df.std()\n",
    "    }\n",
    "\n",
    "    summary_df = pd.DataFrame(summary)\n",
    "    print(\"\\nSummary of 10-Fold Cross-Validation Metrics:\")\n",
    "    print(summary_df.round(4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da941785",
   "metadata": {},
   "source": [
    "#### Applying with training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8405cc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "dt_model = DecisionTreeClassifier(max_depth=14, \n",
    "                                  min_samples_split=0.4,\n",
    "                                  min_samples_leaf=0.2,\n",
    "                                  max_features=10,\n",
    "                                  max_leaf_nodes=25,\n",
    "                                  criterion='entropy'\n",
    "                                  )\n",
    "dt_model.fit(X, y)\n",
    "\n",
    "# Predictions on training fold\n",
    "train_pred = dt_model.predict(X)\n",
    "train_prob = dt_model.predict_proba(X)[:, 1]\n",
    "\n",
    "# Generate classification report for the test set\n",
    "print(classification_report(y, test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1376fc",
   "metadata": {},
   "source": [
    "#### Applying with test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed282cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred = dt_model.predict(test_df)\n",
    "y_pred_proba = dt_model.predict_proba(test_df)[:, 1]\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# Evaluate multiple metrics\n",
    "scoring = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "# Use cross-validation on the training data instead of predictions\n",
    "cv_results = cross_validate(dt_model, X_train, y_train, cv=10, scoring=scoring)\n",
    "# Print results\n",
    "for metric in scoring:\n",
    "    scores = cv_results[f'test_{metric}']\n",
    "    print(f\"{metric.capitalize()} scores: {scores}\")\n",
    "    print(f\"Mean {metric}: {scores.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8c90ab",
   "metadata": {},
   "source": [
    "### 6.  Back Propagation Neural Network (BPNN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
